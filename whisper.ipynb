{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import openai \n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file = open(\"index.mp4\", \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription = client.audio.transcriptions.create(model=\"whisper-1\", file=audio_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, back to the environment here, so we're going to start with importing some of the necessary libraries that we're going to need. So we're going to need JSON, import OS, import pandas as pd, import OpenAI, import AI, import OpenAI. There we go, let's run this. Okay, I'm going to import my key and a few other things and then we'll resume the video. Okay, so we've imported OpenAI, some of the, my key into the environment and started a session with OpenAI. All right, we're gonna, and I have also dragged and dropped this John F. Kennedy inaugural address file into the environment. So let's go ahead and read that in and we'll just call it the audio file for purposes of this. Cursor out of the way, equals open John Kennedy inaugural address. Okay, that needs to be in quotes. Let's see what we got here. Looks like it worked and a few more cells below this. Okay, so let's go ahead and pass this over to the Whisper API and get the transcript back. And a lot of the cloud service providers can do similar services, but we're just trying to keep this as OpenAI centric as possible. So client.audio.transcriptions.create. I'm going to use the whisper-1, comma, file equals that audio file we just passed in. Close the parentheses. Let's see what we get here. Looking pretty good. Okay, so let's see if we got the transcript back. Now when you try and print these, it's a little bit different. Dot text is what you need to do to see and wrap this a print statement. And we're just going to grab a smaller amount just so this is a little bit more manageable. Grab the first hundred characters and see if we got this. Yep, that's got to be him reading off the Bible there. All right, so let's move on to what we're going to prompt the system with. We're going to call this system prompt, and this will be sent over to OpenAI. Let me scroll down a little bit. Zoom in just a little bit. And this will be sent over to OpenAI to kind of prime the model to kind of understand the audio we're sending in. And where I'm actually going to grab this from is right off of the OpenAI documentation. So they have a lot of different documentation. One of these is a key points extraction. So this was, I felt this is kind of the best way to summarize a presidential. So you want to kind of want to get the candidates kind of key points. So this is largely what I grabbed. So I'm just going to kind of copy and paste it, change it slightly to meet our kind of use case. But that's what it is. So efficient AI lists the main points and discuss the product. And this is very, very similar to what's on the OpenAI website. All right. So now that we have that, we can reach out to the chat completion API. Summary, thank you very much. Response equals client.chat.completions.create. Okay, first thing we want is the model. And we will go with GPT-4 here. And then you can change the parameters and a number of different things. Let's go with parameters, basically how often it repeats certain things. So let's go with 25 here. And you can tune these as you see appropriate to your use case. So messages, this, role, colon, system prompt. And I think that's what we want. Okay. Sorry, I stuck scrolling through that. Open and close dictionary here and the role and that. Thank you. Thank you, Copilot. I guessed that correctly. So let's make sure I'm not missing any key bits of syntax here. So if it's running, let that run. Okay, looks like it just finished. So we should have a response back from OpenAI. So response.choices, it's pretty close. except for the line.message.content. Let's see what we're looking at here. Jump. Okay. took the presidential oath faithfully to execute the opposite of preserve, protect, and defend the courts. Not a victory, but a celebration of freedom. Oh. Beliefs and rights of man come from God. Let's see Americans. There's a revolution carry. Okay, pledge loyalty the old offer is special. Pledge to assist Freemans of South America. Need to pledge support to the United Nations. Okay, well, kind of interesting. You could adjust the max tokens here, change the temperature, etc. and get more out of the speech. I believe this is the S not which you can. Yeah, yeah, this is it. Kenny Kennedy concluded by asking Americans not to ask not what their country can do for them, but what they can do for their country and send it the same request citizens all around the world. Yep. I thought that was this speech. And if you mess with the temperature and max tokens, you might get a slightly different response. But the intent of this was kind of meant to be the basis of either a meeting summarizer or something along those lines. Thank\n"
     ]
    }
   ],
   "source": [
    "print(transcription.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"You are a key points extractor. Your task is to identify and list the main points discussed. These points should represent the most important ideas, findings, or topics crucial to the essence of the discussion\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-4\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": transcription.text\n",
    "            }\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. The discussion started with importing necessary libraries such as JSON, OS, pandas, and OpenAI.\n",
      "2. The user imported their key and started a session with OpenAI.\n",
      "3. They read in an audio file of John F. Kennedy's inaugural address.\n",
      "4. The audio file was passed to the Whisper API to get the transcript back.\n",
      "5. The user then created a system prompt to prime the model to understand the audio being sent in.\n",
      "6. The system prompt was based on OpenAI's key points extraction documentation.\n",
      "7. The user then reached out to the chat completion API to get a summary of the speech.\n",
      "8. The response from OpenAI included key points from Kennedy's speech such as his pledge to assist the free men of South America, his support for the United Nations, and his famous request to Americans to ask not what their country can do for them, but what they can do for their country.\n",
      "9. The user suggested that adjusting the max tokens and temperature could yield different responses.\n",
      "10. The overall intent of the discussion was to demonstrate how to use OpenAI for summarizing content, such as a meeting or a speech.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
